{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:60% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking an unknown number of objects\n",
    "\n",
    "While SVI can be used to learn components and assignments of a mixture model, pyro.contrib.tracking provides more efficient inference algorithms to estimate assignments. This notebook demonstrates how to use the `MarginalAssignmentPersistent` with EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch import nn\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.contrib.tracking.assignment import MarginalAssignmentPersistent\n",
    "from pyro.contrib.tracking.hashing import LSH, merge_points\n",
    "from pyro.ops.newton import newton_step\n",
    "from pyro.infer import SVI, TraceEnum_ELBO\n",
    "from pyro.optim import ClippedAdam, ASGD, SGD\n",
    "from pyro.util import warn_if_nan\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "def diag_tensor(tensor):\n",
    "    print(\"shape:{}, mean:{} std:{}, min:{}, max:{}\".format(tensor.shape,tensor.mean(),\n",
    "                                                            tensor.std(),tensor.min(),\n",
    "                                                            tensor.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a global config object to make it easy to port code to `argparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('Args', (object,), {})  # A fake ArgumentParser.parse_args() result.\n",
    "args.num_frames = 40\n",
    "args.max_detections_per_frame = 100\n",
    "args.max_num_objects = 100\n",
    "args.expected_num_objects = 2.\n",
    "\n",
    "args.PNR = 10\n",
    "args.num_sensors=100\n",
    "args.x_min, args.x_max = -2.5, 2.5\n",
    "\n",
    "args.bp_iters = 25\n",
    "args.bp_momentum =0.5\n",
    "args.svi_iters = 201\n",
    "args.em_iters = 10\n",
    "args.merge_radius = -1\n",
    "args.prune_threshold = -1\n",
    "\n",
    "assert args.max_num_objects >= args.expected_num_objects\n",
    "assert args.x_max > args.x_min\n",
    "assert args.max_detections_per_frame >= args.max_num_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a model with deterministic dynamics, say sinusoids with known period but unknown phase and amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamics(num_frames):\n",
    "    time = torch.arange(num_frames,dtype=torch.float)*2*math.pi/num_frames\n",
    "    return torch.stack([time.cos(), time.sin()], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's tricky to define a fully generative model, so instead we'll separate our data generation process `generate_data()` from a factor graph `model()` that will be used in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(args):\n",
    "    num_objects = int(round(args.expected_num_objects))  # Deterministic.\n",
    "        #num_objects = int(dist.Poisson(args.expected_num_objects).sample())\n",
    "    states = dist.Normal(0., 1.).sample((num_objects, 2))\n",
    "    confidence = torch.empty(args.num_frames,args.num_sensors)\n",
    "    positions = get_dynamics(args.num_frames).mm(states.t())\n",
    "    noise_power = 10 ** (-args.PNR/10)\n",
    "    noise_dist = dist.Normal(0, noise_power)\n",
    "    #confidence is number of objects indicating sensor senses object/s\n",
    "    for t in range(args.num_frames):\n",
    "        confidence[t]=torch.histc(positions[t],args.num_sensors,args.x_min,args.x_max)\n",
    "    # if sensors are saturated: can't diff btw 1 object and multiple objects.\n",
    "    confidence[confidence>1] = 1\n",
    "    #AWGN model\n",
    "    sensor_outputs = noise_dist.sample(confidence.shape) + confidence\n",
    "    bin_width=(args.x_max - args.x_min)/args.num_sensors\n",
    "    sensor_positions = torch.arange(args.x_min, args.x_max, bin_width) + bin_width/2\n",
    "    return states, positions, sensor_positions, sensor_outputs, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector\n",
    "This detector has 2 trainable parameters: w and b, where $confidence = sigmoid(wx+b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    # returns confidence of sensor sensing the object\n",
    "    def __init__(self, max_detections_per_frame):\n",
    "        super(Detector, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.max_detections_per_frame = max_detections_per_frame\n",
    "        nn.init.constant_(self.linear.weight, 1.)\n",
    "        nn.init.constant_(self.linear.bias, 0.)\n",
    "        if not self.linear.bias.requires_grad:\n",
    "            print('wtf bias')\n",
    "            self.linear.bias.requires_grad= True\n",
    "        if not self.linear.weight.requires_grad:\n",
    "            print('wtf weight')\n",
    "            self.linear.weight.requires_grad= True\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Detector: w={}, b={}, max_detections_per_frame={}\".format(self.linear.weight.item(),\n",
    "                                                                          self.linear.bias.item(),\n",
    "                                                                          self.max_detections_per_frame)\n",
    "\n",
    "    def forward(self, sensor_positions, sensor_outputs):\n",
    "        # x * w + b\n",
    "        return torch.sigmoid(self.linear((sensor_outputs-0.5).unsqueeze(-1)).squeeze(-1))\n",
    "        \n",
    "detector = Detector(args.max_detections_per_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exists_logits(states_loc, replicates):\n",
    "    FUDGE = -2\n",
    "    # TODO add a term for prior over object location\n",
    "    return states_loc.new_empty(states_loc.shape[0]).fill_(-math.log(replicates) + FUDGE)\n",
    "\n",
    "def compute_assign_logits(positions, observations, replicates, args):\n",
    "    log_likelihood = detection_log_likelihood(positions, observations, args)\n",
    "    assign_logits = log_likelihood[...,:-1] - log_likelihood[...,-1:] - math.log(replicates)\n",
    "    assign_logits[log_likelihood[..., :-1] == -float('inf')] = -float('inf')\n",
    "    #assign_logits -= torch.max(assign_logits, -1, keepdim=True)[0]\n",
    "    return assign_logits\n",
    "\n",
    "def detection_log_likelihood(positions, observations, args):\n",
    "    noise_power = 10 ** (-args.PNR/10)\n",
    "    bin_width = (args.x_max-args.x_min)/args.num_sensors\n",
    "    real_loc_dist = dist.Normal(positions.unsqueeze(-2), bin_width)\n",
    "    real_output_dist = dist.Normal(1., noise_power)\n",
    "    spurious_output_dist = dist.Normal(0., noise_power)\n",
    "    spurious_loc_dist = dist.Uniform(args.x_min, args.x_max)\n",
    "    observed_positions = observations[..., 0].unsqueeze(-1)\n",
    "    observed_outputs = observations[..., 2].unsqueeze(-1)\n",
    "    a = (real_loc_dist.log_prob(observed_positions) +\n",
    "         real_output_dist.log_prob(observed_outputs) + \n",
    "         math.log(args.expected_num_objects)\n",
    "        )\n",
    "    b = (spurious_loc_dist.log_prob(observed_positions) +\n",
    "         spurious_output_dist.log_prob(observed_outputs) +\n",
    "         math.log(args.max_detections_per_frame-args.expected_num_objects)\n",
    "        )\n",
    "    print(\"a:{}, b:{}\".format(a.shape,b.shape))\n",
    "    return torch.cat((a,b), dim=-1) \n",
    "\n",
    "def obs2sensor(obs,args):\n",
    "    sensor_outputs = torch.zeros(args.num_frames,args.num_sensors)\n",
    "    pos2sensoridx= lambda pos: torch.floor((pos - args.x_min)/\n",
    "                                           (args.x_max-args.x_min)*\n",
    "                                           args.num_sensors).long()\n",
    "    for i in range(obs.shape[0]):\n",
    "        for j in range(obs.shape[1]):\n",
    "            if obs[i,j,1]>=0.0:\n",
    "                sensor_outputs[i,pos2sensoridx(obs[i,j,0])] = obs[i,j,2]\n",
    "    return sensor_outputs\n",
    "\n",
    "def sensor2obs(sensor_positions, sensor_outputs, confidence, args):\n",
    "    observations = torch.zeros(sensor_outputs.shape[-2], args.max_detections_per_frame, 3)\n",
    "    for i in range(args.num_frames):\n",
    "        k=0\n",
    "        _ , idx = torch.sort(confidence[i], descending=True)\n",
    "        for j in range(min(sensor_positions.shape[0], int(args.max_detections_per_frame))):\n",
    "            observations[i,j,0] = sensor_positions[idx[j]]\n",
    "            observations[i,j,1] = confidence[i,idx[j]]\n",
    "            observations[i,j,2] = sensor_outputs[i,idx[j]]\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorTracker(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DetectorTracker, self).__init__()\n",
    "        self.detector = Detector(args.max_detections_per_frame)\n",
    "        self.num_objects = args.max_num_objects\n",
    "\n",
    "    @poutine.broadcast\n",
    "    def model(self, sensor_positions, sensor_outputs, args):\n",
    "        bin_width = (args.x_max-args.x_min)/args.num_sensors\n",
    "        pyro.module(\"detectorTracker\", self)\n",
    "        confidence = self.detector.forward(sensor_positions, sensor_outputs)\n",
    "        observations = sensor2obs(sensor_positions, sensor_outputs, confidence, args)\n",
    "        with pyro.iarange(\"objects\", self.num_objects):\n",
    "            exists = pyro.sample(\"exists\",\n",
    "                                 dist.Bernoulli(args.expected_num_objects / self.num_objects),\n",
    "                                 obs = confidence)\n",
    "            with poutine.scale(scale=exists):\n",
    "                states = pyro.sample(\"states\", dist.Normal(0., 1.).expand([2]).independent(1))\n",
    "                positions = get_dynamics(args.num_frames).mm(states.t())\n",
    "\n",
    "        with pyro.iarange(\"time\", args.num_frames):\n",
    "            with pyro.iarange(\"detections\", args.max_detections_per_frame):\n",
    "                # The combinatorial part of the log prob is approximated to allow independence.\n",
    "                assign = pyro.sample(\"assign\",\n",
    "                                     dist.Categorical(torch.ones(self.num_objects)))\n",
    "                is_real= exists[assign]\n",
    "                observed_positions = observations[..., 0]\n",
    "\n",
    "                with poutine.scale(scale=is_real.float()):\n",
    "                    #bogus_position = positions.new_zeros(args.num_frames, 1)\n",
    "                    #augmented_positions = torch.cat([positions, bogus_position], -1)\n",
    "                    predicted_positions = positions[:, assign]\n",
    "                    pyro.sample(\"real_observations\",\n",
    "                                dist.Normal(predicted_positions, bin_width),\n",
    "                                obs=observed_positions)\n",
    "                with poutine.scale(scale=(1-is_real.float())):\n",
    "                    pyro.sample(\"spurious_observations\", dist.Uniform(args.x_min,args.x_max),\n",
    "                                obs=observed_positions)\n",
    "        observation_hat= torch.stack([observed_positions,is_real,is_real], observed_positions.dims())\n",
    "        sensor_hat = obs2sensor(observation_hat,args)\n",
    "        noise_power = 10 ** (-args.PNR/10)\n",
    "        pyro.sample(\"sensor_output\",\n",
    "            dist.Normal(sensor_hat.view(-1),noise_power).independent(1),\n",
    "            obs=sensor_outputs.view(-1))\n",
    "\n",
    "    @poutine.broadcast    \n",
    "    def guide(self, sensor_positions, sensor_outputs, args):\n",
    "        pyro.module('detectorTracker', self)\n",
    "        bin_width = (args.x_max-args.x_min)/args.num_sensors\n",
    "        confidence = self.detector.forward(sensor_positions, sensor_outputs)\n",
    "        observations = sensor2obs(sensor_positions, sensor_outputs, confidence, args)\n",
    "        if observations.dim() == 3:\n",
    "            states_loc = torch.randn(self.num_objects, 2, requires_grad=True)\n",
    "            for em_iter in range(args.em_iters):\n",
    "                states_loc = states_loc.detach()\n",
    "                states_loc.requires_grad = True\n",
    "                positions = get_dynamics(args.num_frames).mm(states_loc.t())\n",
    "                assert states_loc.requires_grad\n",
    "                assert positions.requires_grad\n",
    "                assert positions.grad_fn is not None\n",
    "                replicates = max(1, states_loc.shape[0]/args.expected_num_objects)\n",
    "                # E-step: compute soft assignments\n",
    "                with torch.no_grad():\n",
    "                    assign_logits = compute_assign_logits(positions, observations, replicates, args)\n",
    "                    exists_logits = compute_exists_logits(states_loc, replicates)\n",
    "                    assignment = MarginalAssignmentPersistent(exists_logits, assign_logits,\n",
    "                                                      args.bp_iters, bp_momentum=args.bp_momentum)\n",
    "                    p_exists = assignment.exists_dist.probs\n",
    "                    p_assign = assignment.assign_dist.probs\n",
    "\n",
    "                log_likelihood = detection_log_likelihood(positions, observations, args)\n",
    "                loss = -(log_likelihood * p_assign).sum()\n",
    "                # M-step\n",
    "                states_loc, _ = newton_step(loss, states_loc, bin_width)  \n",
    "\n",
    "                if args.prune_threshold > 0.0:\n",
    "                    states_loc = states_loc[p_exists > args.prune_threshold]\n",
    "                    self.num_objects = states_loc.shape[0]\n",
    "                if args.merge_radius >= 0.0:\n",
    "                    states_loc, _ = merge_points(states_loc, args.merge_radius)\n",
    "                    self.num_objects = states_loc.shape[0]\n",
    "                warn_if_nan(states_loc, 'states_loc')\n",
    "        else:\n",
    "            print(\"Warning: no object detected, observations.shape:{}\".format(observations.shape))\n",
    "\n",
    "        positions = get_dynamics(args.num_frames).mm(states_loc.t())\n",
    "        replicates = max(1, states_loc.shape[0]/args.expected_num_objects)\n",
    "        assign_logits = compute_assign_logits(positions, observations, replicates, args)\n",
    "        exists_logits = compute_exists_logits(states_loc, replicates)\n",
    "        assignment = MarginalAssignmentPersistent(exists_logits, assign_logits,\n",
    "                                          args.bp_iters, bp_momentum=args.bp_momentum)\n",
    "\n",
    "        with pyro.iarange(\"objects\", states_loc.shape[0]):\n",
    "            exists = pyro.sample(\"exists\", assignment.exists_dist, infer={\"enumerate\": \"parallel\"})\n",
    "            with poutine.scale(scale=exists):\n",
    "                pyro.sample(\"states\", dist.Delta(states_loc).independent(1))\n",
    "        with pyro.iarange(\"detections\", observations.shape[1]):\n",
    "            with pyro.iarange(\"time\", args.num_frames):\n",
    "                pyro.sample(\"assign\", assignment.assign_dist, infer={\"enumerate\": \"parallel\"})\n",
    "        return assignment, states_loc, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution(sensor_outputs, assignment, states_loc, observations, args, msg=''):\n",
    "    with torch.no_grad():\n",
    "        assignment, states_loc, observations, gdetector = self.guide(args, sensor_positions, sensor_outputs)\n",
    "    positions = get_dynamics(args.num_frames).mm(states_loc.t())\n",
    "    fig, ax = pyplot.subplots(figsize=(12,9))\n",
    "    fig.patch.set_color('white')\n",
    "    extent = [0-.5, args.num_frames-.5, args.x_min, args.x_max]\n",
    "    cax = ax.matshow(sensor_outputs.t(),aspect='auto',extent=extent, origin='lower',alpha=0.5)\n",
    "    pyplot.colorbar(cax)\n",
    "    pyplot.plot(true_positions.numpy(), 'k--')\n",
    "    is_observed = (observations[..., -2] > args.confidence_threshold)\n",
    "    pos = observations[..., 0]\n",
    "    conf = observations[..., 1]\n",
    "    time = torch.arange(args.num_frames).unsqueeze(-1).expand_as(pos)\n",
    "    pyplot.scatter(time[is_observed].view(-1).numpy(),\n",
    "                   pos[is_observed].detach().view(-1).numpy(), color='k', marker='+',\n",
    "                   s=8*100**conf[is_observed].detach().view(-1).numpy(),\n",
    "                   label='observation')\n",
    "    for i in range(assignment.exists_dist.probs.shape[0]):\n",
    "        p_exists = assignment.exists_dist.probs[i].item()\n",
    "        position = positions[:, i].detach().numpy()\n",
    "        pyplot.plot(position, alpha=p_exists, color='C0')\n",
    "    if args.expected_num_objects == 1:\n",
    "        p_exists = assignment.exists_dist.probs\n",
    "        mean = (p_exists * positions).sum(-1) / p_exists.sum(-1)\n",
    "        pyplot.plot(mean.detach().numpy(), 'r--', alpha=0.5, label='mean')\n",
    "    pyplot.title('Truth, observations, and {:0.1f} predicted tracks {}'.format(\n",
    "                 assignment.exists_dist.probs.sum().item(), message))\n",
    "    pyplot.plot([], 'k--', label='truth')\n",
    "    pyplot.plot([], color='C0', label='prediction')\n",
    "    pyplot.legend(loc='best')\n",
    "    pyplot.xlabel('time step')\n",
    "    pyplot.ylabel('position')\n",
    "    pyplot.tight_layout()\n",
    "    \n",
    "def plot_exists_histogram(p_exists, args):\n",
    "    p_exists = p_exists.detach().numpy()\n",
    "    pyplot.figure(figsize=(6,4)).patch.set_color('white')\n",
    "    pyplot.plot(sorted(p_exists))\n",
    "    pyplot.ylim(0, None)\n",
    "    pyplot.xlim(0, len(p_exists))\n",
    "    pyplot.ylabel('p_exists')\n",
    "    pyplot.xlabel('rank')\n",
    "    pyplot.title('Prob(exists) of {} potential objects, total = {:0.2f}'.format(\n",
    "        len(p_exists), p_exists.sum()))\n",
    "    pyplot.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(0)\n",
    "true_states, true_positions, sensor_positions, sensor_outputs, true_confidence = generate_data(args)\n",
    "true_num_objects = len(true_states)\n",
    "assert true_states.shape == (true_num_objects, 2)\n",
    "assert true_positions.shape == (args.num_frames, true_num_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "10 iterations of EM with and without merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt = DetectorTracker(args)\n",
    "pyro.set_rng_seed(1)  # Use a different seed from data generation\n",
    "pyro.clear_param_store()\n",
    "assignment, states_loc, observations = dt.guide(sensor_positions, sensor_outputs, args)\n",
    "plot_solution(sensor_outputs, assignment, states_loc, observations, args, 'Before training')\n",
    "plot_exists_histogram(assignment.exists_dist.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pyro.set_rng_seed(1)  # Use a different seed from data generation\n",
    "pyro.clear_param_store()\n",
    "infer = SVI(dt.model, dt.guide, ClippedAdam({\"lr\": 0.1}), TraceEnum_ELBO(max_iarange_nesting=2))\n",
    "losses = []\n",
    "for epoch in range(args.svi_iters if not smoke_test else 2):\n",
    "    loss = infer.step(args, sensor_positions, sensor_outputs, true_confidence)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch {: >4d} loss = {}\".format(epoch, loss))\n",
    "    losses.append(loss)\n",
    "pyplot.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)  # Use a different seed from data generation\n",
    "assignment, states_loc, observations = guide(sensor_positions, sensor_outputs, args)\n",
    "plot_solution(sensor_outputs, assignment, states_loc, observations, args, 'After training')\n",
    "plot_exists_histogram(assignment.exists_dist.probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
